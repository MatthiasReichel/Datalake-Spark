{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-e1470d9c": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "# ETL Processes\n",
    "The notebook being used to predfine the ETL process using a small part of the dataset. Afterwards the `etl.py` file is applied loading the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofweek, hour, weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a spark session enabling spark environment.\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Read log and song data from S3\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Extract song data from data source and place it in a dateframe\n",
    "    \n",
    "# Rootpath input data (Amazon S3 bucket). Note: No root path required as relative path sufficient due to local testing\n",
    "# song_data_root = \"s3a://udacity-dend/\"\n",
    "    \n",
    "# Get filepath to data source. Note: Relative path to test data which have been extracted from S3 bucket before.\n",
    "song_data = \"data/song_data/*.json\"\n",
    "log_data = \"data/log_data/*.json\"\n",
    "\n",
    "# Read song data and log data files into respective dataframe schemas\n",
    "df_song_data = spark.read.json(song_data)\n",
    "df_log_data = spark.read.json(log_data)\n",
    "print(\"Success: Read log and song data from S3\")\n",
    "\n",
    "print(df_log_data.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Create dataframe tables\n"
     ]
    }
   ],
   "source": [
    "## Extract data and push them into dataframes\n",
    "\n",
    "# Udf helper function\n",
    "# Extract datetime based on the timestamp from log dataframe\n",
    "get_datetime_udf = udf(lambda ts: datetime.datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Add timestamp column to log data frame as it is needed by songsplay and time dateframe tables\n",
    "df_log_data = df_log_data.withColumn(\"datetime\", get_datetime_udf(df_log_data.ts))\n",
    "\n",
    "# Create Fact table containing all played songs\n",
    "# Combination of songs and artist describing available item\n",
    "# Items which have been actually played are described by calling NextSong page\n",
    "df_songsplay_table = df_log_data.join(df_song_data, (df_log_data.artist == df_song_data.artist_name) & \\\n",
    "                                    (df_log_data.song == df_song_data.title)) \\\n",
    "                                        .select(\"userId\",\n",
    "                                                \"song_Id\",\n",
    "                                                \"artist_Id\",\n",
    "                                                \"sessionId\",\n",
    "                                                \"level\",\n",
    "                                                \"location\",\n",
    "                                                \"userAgent\",\n",
    "                                                \"datetime\",\n",
    "                                                month(\"datetime\").alias(\"month\"),\n",
    "                                                year(\"datetime\").alias(\"year\"),) \\\n",
    "                                                .filter(df_log_data.page == \"NextSong\")\n",
    "\n",
    "# Extract songs columns and modify its datatype to create songs table\n",
    "df_songs_table = df_song_data.selectExpr(\"song_id\", \n",
    "                                         \"title\", \n",
    "                                         \"artist_id\", \n",
    "                                         \"cast(song_id as int) year\", \n",
    "                                         \"cast(song_id as float) duration\")\n",
    "\n",
    "# Extract columns to create artists table\n",
    "df_artists_table = df_song_data.selectExpr(\"artist_id\", \n",
    "                                           \"artist_name\", \n",
    "                                           \"artist_location\", \n",
    "                                           \"cast(artist_latitude as float) artist_latitude\", \n",
    "                                           \"cast(artist_longitude as float) artist_longitude\") \\\n",
    "                                           .dropDuplicates([\"artist_id\"])\n",
    "\n",
    "# Extract user columns to create users table\n",
    "df_users_table = df_log_data.select(\"userId\", \n",
    "                                    \"firstName\", \n",
    "                                    \"lastName\", \n",
    "                                    \"gender\", \n",
    "                                    \"level\") \\\n",
    "                                    .dropDuplicates([\"userId\"])\n",
    "\n",
    "# Create time table dataframe for all songs which have been played\n",
    "df_time_table = df_songsplay_table.select([hour(\"datetime\").alias(\"hour\"),\n",
    "                                          month(\"datetime\").alias(\"month\"),\n",
    "                                          year(\"datetime\").alias(\"year\"),\n",
    "                                          dayofweek(\"datetime\").alias(\"weekday\"),\n",
    "                                          weekofyear(\"datetime\").alias(\"weekofyear\"),\n",
    "                                          \"datetime\"])\n",
    "\n",
    "print(\"Success: Create dataframe tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframes (tables) as parquet files \n",
    "\n",
    "# Rootpath output data (Amazon S3 bucket). Note: No root path required as relative path sufficient due to local testing\n",
    "# output_data_root = \"s3a://s3-bucket-udacity/\"\n",
    "\n",
    "# Write songs back to parquet files partitioned by year and artist id\n",
    "songs_table_parquet = df_songs_table.write.parquet(partitionBy=[\"year\",\"artist_id\"], \n",
    "                                                   path=\"data/song_data/songs.parquet\", \n",
    "                                                   mode=\"overwrite\")\n",
    "\n",
    "# Write users back to parquet file\n",
    "users_table_parquet = df_users_table.write.parquet(path=\"data/song_data/users.parquet\", \n",
    "                                                   mode=\"overwrite\")\n",
    "\n",
    "# Write artists back to parquet file\n",
    "artists_table_parquet = df_artists_table.write.parquet(path=\"data/song_data/artists.parquet\", \n",
    "                                                       mode=\"overwrite\")\n",
    "\n",
    "# Write played songs back to parquet file partitioned by year and month\n",
    "songplays_table_parquet = df_songsplay_table.write.parquet(partitionBy=[\"year\",\"month\"], \n",
    "                                                           path=\"data/songsplay_data/songsplay.parquet\", \n",
    "                                                           mode=\"overwrite\")\n",
    "\n",
    "# Write time details back to parquet file partitioned by year and month\n",
    "time_table_parquet = df_time_table.write.parquet(partitionBy=[\"year\",\"month\"], \n",
    "                                                 path=\"data/time_data/time.parquet\", \n",
    "                                                 mode=\"overwrite\")\n",
    "\n",
    "print(\"Success: Write data into parquet files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
